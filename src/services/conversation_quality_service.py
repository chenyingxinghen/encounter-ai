"""å¯¹è¯è´¨é‡ç›‘æµ‹æœåŠ¡"""
import uuid
import re
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from collections import Counter
from src.models.conversation import Conversation, Message
from src.models.quality import (
    QualityMetrics,
    ConversationReport,
    SatisfactionFeedback,
    QualityMonitoringRequest,
    SatisfactionFeedbackRequest,
    TopicSegment
)
from src.services.conversation_service import ConversationService
from src.utils.logger import get_logger
from src.utils.exceptions import ConversationNotFoundError

logger = get_logger(__name__)


class ConversationQualityService:
    """å¯¹è¯è´¨é‡ç›‘æµ‹æœåŠ¡ç±»"""
    
    def __init__(self, conversation_service: ConversationService):
        """
        åˆå§‹åŒ–å¯¹è¯è´¨é‡ç›‘æµ‹æœåŠ¡
        
        Args:
            conversation_service: å¯¹è¯æœåŠ¡å®ä¾‹
        """
        self.conversation_service = conversation_service
        self.feedback_storage: Dict[str, List[SatisfactionFeedback]] = {}
        self.reports_storage: Dict[str, ConversationReport] = {}
        
        # è´¨é‡é˜ˆå€¼é…ç½®
        self.LOW_QUALITY_THRESHOLD = 5.0  # æ•´ä½“è´¨é‡å¾—åˆ†ä½äº5.0è§†ä¸ºä½è´¨é‡
        self.MIN_MESSAGES_FOR_ANALYSIS = 10  # è‡³å°‘10æ¡æ¶ˆæ¯æ‰è¿›è¡Œåˆ†æ
        
        logger.info("ConversationQualityService initialized")
    
    def analyze_topic_depth(
        self,
        messages: List[Message]
    ) -> Tuple[float, int, float]:
        """
        åˆ†æè¯é¢˜æ·±åº¦
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            Tuple[float, int, float]: (è¯é¢˜æ·±åº¦å¾—åˆ†, è¯é¢˜æ•°é‡, å¹³å‡è¯é¢˜æŒç»­æ—¶é—´)
        """
        if len(messages) < 2:
            return 0.0, 0, 0.0
        
        # æå–è¯é¢˜ç‰‡æ®µ
        topic_segments = self._segment_topics(messages)
        
        if not topic_segments:
            return 0.0, 0, 0.0
        
        # è®¡ç®—è¯é¢˜æ·±åº¦å¾—åˆ†
        total_depth = sum(segment.depth_score for segment in topic_segments)
        avg_depth = total_depth / len(topic_segments)
        
        # è®¡ç®—å¹³å‡è¯é¢˜æŒç»­æ—¶é—´
        total_duration = sum(segment.message_count for segment in topic_segments)
        avg_duration = total_duration / len(topic_segments)
        
        # è¯é¢˜æ·±åº¦å¾—åˆ† = å¹³å‡æ·±åº¦ * (1 + log(è¯é¢˜æ•°é‡))
        # é¼“åŠ±å¤šæ ·åŒ–çš„è¯é¢˜ï¼Œä½†ä¸è¿‡åˆ†æƒ©ç½šä¸“æ³¨çš„å¯¹è¯
        import math
        topic_diversity_factor = 1 + math.log(len(topic_segments) + 1) / 10
        final_score = min(10.0, avg_depth * topic_diversity_factor)
        
        logger.info(
            f"Topic depth analysis: score={final_score:.2f}, "
            f"topics={len(topic_segments)}, avg_duration={avg_duration:.2f}"
        )
        
        return final_score, len(topic_segments), avg_duration
    
    def _segment_topics(self, messages: List[Message]) -> List[TopicSegment]:
        """
        å°†å¯¹è¯åˆ†å‰²æˆè¯é¢˜ç‰‡æ®µ
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            List[TopicSegment]: è¯é¢˜ç‰‡æ®µåˆ—è¡¨
        """
        if len(messages) < 2:
            return []
        
        segments = []
        current_segment_start = 0
        current_keywords = self._extract_keywords(messages[0].content)
        
        for i in range(1, len(messages)):
            msg_keywords = self._extract_keywords(messages[i].content)
            
            # è®¡ç®—å…³é”®è¯é‡å åº¦
            overlap = len(set(current_keywords) & set(msg_keywords))
            similarity = overlap / max(len(current_keywords), len(msg_keywords), 1)
            
            # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œè®¤ä¸ºè¯é¢˜å‘ç”Ÿäº†è½¬æ¢
            if similarity < 0.3 or i - current_segment_start > 20:
                # ä¿å­˜å½“å‰ç‰‡æ®µ
                segment_messages = messages[current_segment_start:i]
                depth_score = self._calculate_segment_depth(segment_messages)
                
                segment = TopicSegment(
                    topic_id=str(uuid.uuid4()),
                    conversation_id=messages[0].conversation_id,
                    start_message_index=current_segment_start,
                    end_message_index=i - 1,
                    topic_keywords=current_keywords[:5],  # ä¿ç•™å‰5ä¸ªå…³é”®è¯
                    depth_score=depth_score,
                    message_count=i - current_segment_start
                )
                segments.append(segment)
                
                # å¼€å§‹æ–°ç‰‡æ®µ
                current_segment_start = i
                current_keywords = msg_keywords
            else:
                # åˆå¹¶å…³é”®è¯
                current_keywords = list(set(current_keywords + msg_keywords))
        
        # ä¿å­˜æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_segment_start < len(messages):
            segment_messages = messages[current_segment_start:]
            depth_score = self._calculate_segment_depth(segment_messages)
            
            segment = TopicSegment(
                topic_id=str(uuid.uuid4()),
                conversation_id=messages[0].conversation_id,
                start_message_index=current_segment_start,
                end_message_index=len(messages) - 1,
                topic_keywords=current_keywords[:5],
                depth_score=depth_score,
                message_count=len(messages) - current_segment_start
            )
            segments.append(segment)
        
        return segments
    
    def _extract_keywords(self, text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        # ç®€å•çš„å…³é”®è¯æå–ï¼šç§»é™¤åœç”¨è¯ï¼Œæå–é•¿åº¦>=2çš„è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬',
                     'è¿™', 'é‚£', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸',
                     'å—', 'å‘¢', 'å§', 'å•Š', 'å“¦', 'å—¯', 'å“ˆ', 'å‘€'}
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†è¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
        words = re.findall(r'[\u4e00-\u9fa5]+', text)
        keywords = [w for w in words if len(w) >= 2 and w not in stopwords]
        
        return keywords
    
    def _calculate_segment_depth(self, messages: List[Message]) -> float:
        """
        è®¡ç®—è¯é¢˜ç‰‡æ®µçš„æ·±åº¦å¾—åˆ†
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            float: æ·±åº¦å¾—åˆ† (0-10)
        """
        if not messages:
            return 0.0
        
        # æ·±åº¦æŒ‡æ ‡ï¼š
        # 1. æ¶ˆæ¯é•¿åº¦ï¼ˆæ›´é•¿çš„æ¶ˆæ¯è¡¨ç¤ºæ›´æ·±å…¥çš„è®¨è®ºï¼‰
        avg_length = sum(len(msg.content) for msg in messages) / len(messages)
        length_score = min(10.0, avg_length / 10)  # 100å­—ä¸ºæ»¡åˆ†
        
        # 2. æ¶ˆæ¯æ•°é‡ï¼ˆæ›´å¤šçš„æ¶ˆæ¯è¡¨ç¤ºæŒç»­çš„è®¨è®ºï¼‰
        count_score = min(10.0, len(messages) / 2)  # 20æ¡æ¶ˆæ¯ä¸ºæ»¡åˆ†
        
        # 3. è¯æ±‡å¤šæ ·æ€§
        all_words = []
        for msg in messages:
            all_words.extend(self._extract_keywords(msg.content))
        
        unique_words = len(set(all_words))
        total_words = len(all_words)
        diversity_score = (unique_words / max(total_words, 1)) * 10 if total_words > 0 else 0
        
        # ç»¼åˆå¾—åˆ†
        depth_score = (length_score * 0.4 + count_score * 0.3 + diversity_score * 0.3)
        
        return min(10.0, depth_score)
    
    def analyze_response_consistency(
        self,
        messages: List[Message],
        conversation: Conversation
    ) -> Tuple[float, float, float]:
        """
        åˆ†æå›åº”ä¸€è‡´æ€§
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            conversation: å¯¹è¯å¯¹è±¡
            
        Returns:
            Tuple[float, float, float]: (å›åº”ä¸€è‡´æ€§å¾—åˆ†, å¹³å‡å›åº”æ—¶é—´, å›åº”é•¿åº¦æ–¹å·®)
        """
        if len(messages) < 2:
            return 0.0, 0.0, 0.0
        
        # è®¡ç®—å›åº”æ—¶é—´
        response_times = []
        for i in range(1, len(messages)):
            if messages[i].sender_id != messages[i-1].sender_id:
                time_diff = (messages[i].timestamp - messages[i-1].timestamp).total_seconds()
                response_times.append(time_diff)
        
        avg_response_time = sum(response_times) / len(response_times) if response_times else 0.0
        
        # è®¡ç®—å›åº”é•¿åº¦æ–¹å·®
        message_lengths = [len(msg.content) for msg in messages]
        avg_length = sum(message_lengths) / len(message_lengths)
        variance = sum((l - avg_length) ** 2 for l in message_lengths) / len(message_lengths)
        
        # è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†
        # 1. å›åº”æ—¶é—´ä¸€è‡´æ€§ï¼ˆå›åº”æ—¶é—´è¶Šç¨³å®šè¶Šå¥½ï¼‰
        if response_times:
            time_variance = sum((t - avg_response_time) ** 2 for t in response_times) / len(response_times)
            time_consistency = 1.0 / (1.0 + time_variance / 100)  # å½’ä¸€åŒ–
        else:
            time_consistency = 0.0
        
        # 2. å›åº”é•¿åº¦ä¸€è‡´æ€§ï¼ˆé•¿åº¦è¶Šç¨³å®šè¶Šå¥½ï¼‰
        length_consistency = 1.0 / (1.0 + variance / 1000)  # å½’ä¸€åŒ–
        
        # 3. äº¤äº’å¹³è¡¡æ€§ï¼ˆåŒæ–¹æ¶ˆæ¯æ•°é‡åº”è¯¥ç›¸å¯¹å¹³è¡¡ï¼‰
        user_a_count = sum(1 for msg in messages if msg.sender_id == conversation.user_a_id)
        user_b_count = sum(1 for msg in messages if msg.sender_id == conversation.user_b_id)
        balance = min(user_a_count, user_b_count) / max(user_a_count, user_b_count, 1)
        
        # ç»¼åˆå¾—åˆ†
        consistency_score = (time_consistency * 0.3 + length_consistency * 0.3 + balance * 0.4)
        
        logger.info(
            f"Response consistency analysis: score={consistency_score:.2f}, "
            f"avg_response_time={avg_response_time:.2f}s, variance={variance:.2f}"
        )
        
        return consistency_score, avg_response_time, variance
    
    def analyze_emotion_sync(
        self,
        messages: List[Message]
    ) -> Tuple[float, float]:
        """
        åˆ†ææƒ…æ„ŸåŒæ­¥æ€§
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            Tuple[float, float]: (æƒ…æ„ŸåŒæ­¥æ€§å¾—åˆ†, æƒ…ç»ªä¸€è‡´ç‡)
        """
        if len(messages) < 2:
            return 0.0, 0.0
        
        # ä¸ºæ²¡æœ‰æƒ…æ„Ÿæ ‡æ³¨çš„æ¶ˆæ¯è¿›è¡Œç®€å•çš„æƒ…æ„Ÿåˆ†æ
        messages_with_emotion = self._analyze_emotions(messages)
        
        # è®¡ç®—ç›¸é‚»æ¶ˆæ¯çš„æƒ…æ„Ÿä¸€è‡´æ€§
        emotion_matches = 0
        total_pairs = 0
        
        for i in range(1, len(messages_with_emotion)):
            if messages_with_emotion[i].sender_id != messages_with_emotion[i-1].sender_id:
                emotion1 = messages_with_emotion[i-1].emotion
                emotion2 = messages_with_emotion[i].emotion
                
                if emotion1 and emotion2:
                    # æƒ…æ„ŸåŒ¹é…è§„åˆ™
                    if emotion1 == emotion2:
                        emotion_matches += 1.0
                    elif self._emotions_compatible(emotion1, emotion2):
                        emotion_matches += 0.5
                    
                    total_pairs += 1
        
        # è®¡ç®—æƒ…ç»ªä¸€è‡´ç‡
        alignment_rate = emotion_matches / total_pairs if total_pairs > 0 else 0.0
        
        # è®¡ç®—æƒ…æ„ŸåŒæ­¥æ€§å¾—åˆ†
        # è€ƒè™‘æƒ…ç»ªä¸€è‡´ç‡å’Œæƒ…ç»ªå¼ºåº¦çš„åè°ƒæ€§
        emotion_sync_score = alignment_rate
        
        logger.info(
            f"Emotion sync analysis: score={emotion_sync_score:.2f}, "
            f"alignment_rate={alignment_rate:.2f}"
        )
        
        return emotion_sync_score, alignment_rate
    
    def _analyze_emotions(self, messages: List[Message]) -> List[Message]:
        """
        ä¸ºæ¶ˆæ¯åˆ†ææƒ…æ„Ÿï¼ˆå¦‚æœå°šæœªæ ‡æ³¨ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            
        Returns:
            List[Message]: å¸¦æœ‰æƒ…æ„Ÿæ ‡æ³¨çš„æ¶ˆæ¯åˆ—è¡¨
        """
        # ç®€å•çš„åŸºäºå…³é”®è¯çš„æƒ…æ„Ÿåˆ†æ
        positive_keywords = ['å¼€å¿ƒ', 'é«˜å…´', 'å¿«ä¹', 'å–œæ¬¢', 'æ£’', 'å¥½', 'èµ', 'å“ˆå“ˆ', 'ğŸ˜Š', 'ğŸ‘']
        negative_keywords = ['éš¾è¿‡', 'ä¼¤å¿ƒ', 'å¤±æœ›', 'ç³Ÿç³•', 'å·®', 'çƒ¦', 'ç´¯', 'ğŸ˜¢', 'ğŸ˜']
        anxious_keywords = ['æ‹…å¿ƒ', 'ç„¦è™‘', 'ç´§å¼ ', 'å®³æ€•', 'ä¸å®‰', 'å‹åŠ›', 'ğŸ˜°', 'ğŸ˜¨']
        
        result_messages = []
        for msg in messages:
            if msg.emotion is None:
                # åˆ†ææƒ…æ„Ÿ
                content_lower = msg.content.lower()
                
                positive_count = sum(1 for kw in positive_keywords if kw in content_lower)
                negative_count = sum(1 for kw in negative_keywords if kw in content_lower)
                anxious_count = sum(1 for kw in anxious_keywords if kw in content_lower)
                
                # ç¡®å®šä¸»å¯¼æƒ…æ„Ÿ
                if anxious_count > 0:
                    emotion = 'anxious'
                    intensity = min(1.0, anxious_count / 3)
                elif positive_count > negative_count:
                    emotion = 'positive'
                    intensity = min(1.0, positive_count / 3)
                elif negative_count > positive_count:
                    emotion = 'negative'
                    intensity = min(1.0, negative_count / 3)
                else:
                    emotion = 'neutral'
                    intensity = 0.5
                
                # åˆ›å»ºæ–°çš„æ¶ˆæ¯å¯¹è±¡ï¼ˆå¸¦æœ‰æƒ…æ„Ÿæ ‡æ³¨ï¼‰
                msg_dict = msg.dict()
                msg_dict['emotion'] = emotion
                msg_dict['emotion_intensity'] = intensity
                result_messages.append(Message(**msg_dict))
            else:
                result_messages.append(msg)
        
        return result_messages
    
    def _emotions_compatible(self, emotion1: str, emotion2: str) -> bool:
        """
        åˆ¤æ–­ä¸¤ç§æƒ…æ„Ÿæ˜¯å¦å…¼å®¹
        
        Args:
            emotion1: æƒ…æ„Ÿ1
            emotion2: æƒ…æ„Ÿ2
            
        Returns:
            bool: æ˜¯å¦å…¼å®¹
        """
        # å®šä¹‰å…¼å®¹çš„æƒ…æ„Ÿå¯¹
        compatible_pairs = {
            ('positive', 'neutral'),
            ('neutral', 'positive'),
            ('negative', 'anxious'),
            ('anxious', 'negative'),
        }
        
        return (emotion1, emotion2) in compatible_pairs
    
    def monitor_conversation_quality(
        self,
        request: QualityMonitoringRequest
    ) -> QualityMetrics:
        """
        å®æ—¶ç›‘æµ‹å¯¹è¯è´¨é‡
        
        Args:
            request: è´¨é‡ç›‘æµ‹è¯·æ±‚
            
        Returns:
            QualityMetrics: è´¨é‡æŒ‡æ ‡
            
        Raises:
            ConversationNotFoundError: å¯¹è¯ä¸å­˜åœ¨
        """
        # è·å–å¯¹è¯ä¿¡æ¯
        conversation = self.conversation_service.get_conversation(request.conversation_id)
        
        # è·å–å¯¹è¯å†å²
        from src.models.conversation import ConversationHistoryRequest
        history_request = ConversationHistoryRequest(
            conversation_id=request.conversation_id,
            limit=500
        )
        messages = self.conversation_service.get_conversation_history(history_request)
        messages = list(reversed(messages))  # æŒ‰æ—¶é—´æ­£åº
        
        if len(messages) < self.MIN_MESSAGES_FOR_ANALYSIS:
            # æ¶ˆæ¯å¤ªå°‘ï¼Œè¿”å›é»˜è®¤æŒ‡æ ‡
            logger.warning(
                f"Conversation {request.conversation_id} has too few messages "
                f"({len(messages)}) for quality analysis"
            )
            return QualityMetrics(
                conversation_id=request.conversation_id,
                topic_depth_score=0.0,
                topic_count=0,
                average_topic_duration=0.0,
                response_consistency_score=0.0,
                average_response_time=0.0,
                response_length_variance=0.0,
                emotion_sync_score=0.0,
                emotion_alignment_rate=0.0,
                overall_quality_score=0.0
            )
        
        # åˆ†æè¯é¢˜æ·±åº¦
        topic_depth_score, topic_count, avg_topic_duration = self.analyze_topic_depth(messages)
        
        # åˆ†æå›åº”ä¸€è‡´æ€§
        response_consistency_score, avg_response_time, response_variance = \
            self.analyze_response_consistency(messages, conversation)
        
        # åˆ†ææƒ…æ„ŸåŒæ­¥æ€§
        emotion_sync_score, emotion_alignment_rate = self.analyze_emotion_sync(messages)
        
        # è®¡ç®—æ•´ä½“è´¨é‡å¾—åˆ†
        overall_quality_score = (
            topic_depth_score * 0.4 +
            response_consistency_score * 10 * 0.3 +
            emotion_sync_score * 10 * 0.3
        )
        
        metrics = QualityMetrics(
            conversation_id=request.conversation_id,
            topic_depth_score=topic_depth_score,
            topic_count=topic_count,
            average_topic_duration=avg_topic_duration,
            response_consistency_score=response_consistency_score,
            average_response_time=avg_response_time,
            response_length_variance=response_variance,
            emotion_sync_score=emotion_sync_score,
            emotion_alignment_rate=emotion_alignment_rate,
            overall_quality_score=overall_quality_score
        )
        
        # æ›´æ–°å¯¹è¯çš„è´¨é‡æŒ‡æ ‡
        self.conversation_service.update_quality_metrics(
            conversation_id=request.conversation_id,
            topic_depth_score=topic_depth_score,
            emotion_sync_score=emotion_sync_score
        )
        
        logger.info(
            f"Quality metrics calculated for conversation {request.conversation_id}: "
            f"overall_score={overall_quality_score:.2f}"
        )
        
        return metrics
    
    def generate_conversation_report(
        self,
        conversation_id: str
    ) -> ConversationReport:
        """
        ç”Ÿæˆå¯¹è¯è´¨é‡æŠ¥å‘Š
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            ConversationReport: å¯¹è¯è´¨é‡æŠ¥å‘Š
            
        Raises:
            ConversationNotFoundError: å¯¹è¯ä¸å­˜åœ¨
        """
        # è·å–å¯¹è¯ä¿¡æ¯
        conversation = self.conversation_service.get_conversation(conversation_id)
        
        # ç›‘æµ‹è´¨é‡æŒ‡æ ‡
        request = QualityMonitoringRequest(conversation_id=conversation_id)
        metrics = self.monitor_conversation_quality(request)
        
        # è®¡ç®—å¯¹è¯æ—¶é•¿
        if conversation.ended_at:
            duration = (conversation.ended_at - conversation.started_at).total_seconds()
        else:
            duration = (datetime.now() - conversation.started_at).total_seconds()
        
        # è·å–æ»¡æ„åº¦åé¦ˆ
        feedbacks = self.feedback_storage.get(conversation_id, [])
        user_a_satisfaction = None
        user_b_satisfaction = None
        
        for feedback in feedbacks:
            if feedback.user_id == conversation.user_a_id:
                user_a_satisfaction = feedback.satisfaction_score
            elif feedback.user_id == conversation.user_b_id:
                user_b_satisfaction = feedback.satisfaction_score
        
        # ç”Ÿæˆå»ºè®®
        suggestions = self._generate_suggestions(metrics, conversation)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºä½è´¨é‡å¯¹è¯
        is_low_quality = metrics.overall_quality_score < self.LOW_QUALITY_THRESHOLD
        
        # åˆ›å»ºæŠ¥å‘Š
        report_id = str(uuid.uuid4())
        report = ConversationReport(
            report_id=report_id,
            conversation_id=conversation_id,
            user_a_id=conversation.user_a_id,
            user_b_id=conversation.user_b_id,
            scene=conversation.scene,
            started_at=conversation.started_at,
            ended_at=conversation.ended_at,
            duration_seconds=duration,
            message_count=conversation.message_count,
            metrics=metrics,
            user_a_satisfaction=user_a_satisfaction,
            user_b_satisfaction=user_b_satisfaction,
            suggestions=suggestions,
            is_low_quality=is_low_quality
        )
        
        # å­˜å‚¨æŠ¥å‘Š
        self.reports_storage[conversation_id] = report
        
        logger.info(
            f"Generated conversation report {report_id} for conversation {conversation_id}"
        )
        
        return report
    
    def _generate_suggestions(
        self,
        metrics: QualityMetrics,
        conversation: Conversation
    ) -> List[str]:
        """
        æ ¹æ®è´¨é‡æŒ‡æ ‡ç”Ÿæˆæ”¹è¿›å»ºè®®
        
        Args:
            metrics: è´¨é‡æŒ‡æ ‡
            conversation: å¯¹è¯å¯¹è±¡
            
        Returns:
            List[str]: å»ºè®®åˆ—è¡¨
        """
        suggestions = []
        
        # è¯é¢˜æ·±åº¦å»ºè®®
        if metrics.topic_depth_score < 5.0:
            suggestions.append("å°è¯•æ·±å…¥æ¢è®¨æ„Ÿå…´è¶£çš„è¯é¢˜ï¼Œåˆ†äº«æ›´å¤šç»†èŠ‚å’Œä¸ªäººç»å†")
        
        if metrics.topic_count < 3:
            suggestions.append("å¯ä»¥å°è¯•æ¢ç´¢æ›´å¤šä¸åŒçš„è¯é¢˜ï¼Œå¢åŠ å¯¹è¯çš„å¤šæ ·æ€§")
        
        # å›åº”ä¸€è‡´æ€§å»ºè®®
        if metrics.response_consistency_score < 0.5:
            suggestions.append("ä¿æŒç¨³å®šçš„å›åº”èŠ‚å¥ï¼Œè®©å¯¹è¯æ›´åŠ æµç•…")
        
        if metrics.average_response_time > 60:
            suggestions.append("å°½é‡åŠæ—¶å›å¤æ¶ˆæ¯ï¼Œé¿å…å¯¹è¯ä¸­æ–­")
        
        # æƒ…æ„ŸåŒæ­¥æ€§å»ºè®®
        if metrics.emotion_sync_score < 0.5:
            suggestions.append("å¤šå…³æ³¨å¯¹æ–¹çš„æƒ…ç»ªçŠ¶æ€ï¼Œç»™äºˆé€‚å½“çš„æƒ…æ„Ÿå›åº”")
        
        # æ•´ä½“è´¨é‡å»ºè®®
        if metrics.overall_quality_score < 5.0:
            suggestions.append("å½“å‰å¯¹è¯è´¨é‡è¾ƒä½ï¼Œå»ºè®®å°è¯•å…¶ä»–åŒ¹é…å¯¹è±¡")
        
        # åœºæ™¯ç‰¹å®šå»ºè®®
        if conversation.scene == 'è€ƒç ”è‡ªä¹ å®¤':
            if metrics.topic_depth_score < 6.0:
                suggestions.append("å¯ä»¥åˆ†äº«æ›´å¤šå­¦ä¹ æ–¹æ³•å’Œå¤‡è€ƒç»éªŒ")
        elif conversation.scene == 'å¿ƒç†æ ‘æ´':
            if metrics.emotion_sync_score < 0.6:
                suggestions.append("å¤šç»™äºˆå¯¹æ–¹æƒ…æ„Ÿæ”¯æŒå’Œå…±æƒ…")
        
        return suggestions
    
    def collect_satisfaction_feedback(
        self,
        request: SatisfactionFeedbackRequest
    ) -> SatisfactionFeedback:
        """
        æ”¶é›†æ»¡æ„åº¦åé¦ˆ
        
        Args:
            request: æ»¡æ„åº¦åé¦ˆè¯·æ±‚
            
        Returns:
            SatisfactionFeedback: æ»¡æ„åº¦åé¦ˆå¯¹è±¡
            
        Raises:
            ConversationNotFoundError: å¯¹è¯ä¸å­˜åœ¨
        """
        # éªŒè¯å¯¹è¯å­˜åœ¨
        conversation = self.conversation_service.get_conversation(request.conversation_id)
        
        # éªŒè¯ç”¨æˆ·æ˜¯å¯¹è¯å‚ä¸è€…
        if request.user_id not in [conversation.user_a_id, conversation.user_b_id]:
            from src.utils.exceptions import UnauthorizedAccessError
            raise UnauthorizedAccessError(
                f"User {request.user_id} is not a participant in conversation {request.conversation_id}"
            )
        
        # åˆ›å»ºåé¦ˆ
        feedback_id = str(uuid.uuid4())
        feedback = SatisfactionFeedback(
            feedback_id=feedback_id,
            conversation_id=request.conversation_id,
            user_id=request.user_id,
            satisfaction_score=request.satisfaction_score,
            feedback_text=request.feedback_text,
            feedback_tags=request.feedback_tags
        )
        
        # å­˜å‚¨åé¦ˆ
        if request.conversation_id not in self.feedback_storage:
            self.feedback_storage[request.conversation_id] = []
        self.feedback_storage[request.conversation_id].append(feedback)
        
        # æ›´æ–°å¯¹è¯çš„æ»¡æ„åº¦å¾—åˆ†
        self.conversation_service.update_quality_metrics(
            conversation_id=request.conversation_id,
            satisfaction_score=request.satisfaction_score
        )
        
        logger.info(
            f"Collected satisfaction feedback {feedback_id} from user {request.user_id} "
            f"for conversation {request.conversation_id}: score={request.satisfaction_score}"
        )
        
        return feedback
    
    def detect_low_quality_conversation(
        self,
        conversation_id: str
    ) -> Tuple[bool, List[str]]:
        """
        æ£€æµ‹ä½è´¨é‡å¯¹è¯å¹¶æä¾›å»ºè®®
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            Tuple[bool, List[str]]: (æ˜¯å¦ä¸ºä½è´¨é‡å¯¹è¯, å»ºè®®åˆ—è¡¨)
            
        Raises:
            ConversationNotFoundError: å¯¹è¯ä¸å­˜åœ¨
        """
        # ç›‘æµ‹è´¨é‡æŒ‡æ ‡
        request = QualityMonitoringRequest(conversation_id=conversation_id)
        metrics = self.monitor_conversation_quality(request)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºä½è´¨é‡å¯¹è¯
        is_low_quality = metrics.overall_quality_score < self.LOW_QUALITY_THRESHOLD
        
        # ç”Ÿæˆå»ºè®®
        conversation = self.conversation_service.get_conversation(conversation_id)
        suggestions = self._generate_suggestions(metrics, conversation)
        
        if is_low_quality:
            suggestions.append("å»ºè®®å°è¯•å…¶ä»–åŒ¹é…å¯¹è±¡ï¼Œå¯»æ‰¾æ›´åˆé€‚çš„äº¤æµä¼™ä¼´")
        
        logger.info(
            f"Low quality detection for conversation {conversation_id}: "
            f"is_low_quality={is_low_quality}, overall_score={metrics.overall_quality_score:.2f}"
        )
        
        return is_low_quality, suggestions
    
    def get_conversation_report(self, conversation_id: str) -> Optional[ConversationReport]:
        """
        è·å–å¯¹è¯è´¨é‡æŠ¥å‘Š
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            Optional[ConversationReport]: å¯¹è¯è´¨é‡æŠ¥å‘Šï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        return self.reports_storage.get(conversation_id)
    
    def get_user_feedbacks(self, conversation_id: str) -> List[SatisfactionFeedback]:
        """
        è·å–å¯¹è¯çš„æ‰€æœ‰æ»¡æ„åº¦åé¦ˆ
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            List[SatisfactionFeedback]: æ»¡æ„åº¦åé¦ˆåˆ—è¡¨
        """
        return self.feedback_storage.get(conversation_id, [])
